# AI Fairness: How to Measure and Reduce Unwanted Bias in Machine Learning (Mahoney, Varshney, and Hind). 

AIF 360 Toolkit: https://github.com/Trusted-AI/AIF360

Book - https://krvarshney.github.io/pubs/MahoneyVH2020.pdf

---

## Table of Contents

1. **Overview of the Paper**  
2. **Introduction and Motivation**  
   - What Is AI Bias and Why Should We Care?  
   - The Challenge of Defining Fairness  
   - Where Does Bias Come From in Machine Learning?  
3. **Core Concepts and Terminology**  
   - Protected Attributes, Privileged vs. Unprivileged  
   - Individual vs. Group Fairness  
   - “We’re All Equal” (WAE) vs. “What You See Is What You Get” (WYSIWYG)  
   - Key Fairness Metrics  
4. **AIF360: Measuring Bias**  
   - Overview of AIF360 and Its Goals  
   - Classes of Fairness Metrics  
   - Which Metrics Should You Use?  
5. **AIF360: Mitigating Bias**  
   - Pre-Processing Algorithms  
   - In-Processing Algorithms  
   - Post-Processing Algorithms  
   - Continuous Monitoring  
6. **Step-by-Step Code Tutorial**  
   - Python Environment Setup  
   - Example with the *Adult* Census Dataset  
   - Measuring Bias Before Mitigation  
   - Mitigating Bias with a Pre-Processing Algorithm  
   - Measuring Bias After Mitigation  
7. **Conclusion & Future Directions**  

---

## 1. Overview of the Paper

The paper *AI Fairness: How to Measure and Reduce Unwanted Bias in Machine Learning* (published by IBM/O’Reilly) explores:

> *“…why data science teams need to engage early and authoritatively on building trusted artificial intelligence (AI)... how organizations should think about AI fairness, as well as the trade-offs between model bias and model accuracy.”*

In simpler terms, the authors highlight:

- **Why** AI bias is such a pressing concern across industries.  
- **What** definitions of fairness exist, and how they differ.  
- **How** to measure unfair bias in a dataset or model.  
- **How** to mitigate that bias with various approaches in different parts of the ML pipeline (pre-, in-, and post-processing).  
- **An introduction** to the IBM AIF360 (AI Fairness 360) toolkit, including sample code for bias detection and mitigation.  

---

## 2. Introduction and Motivation

### What Is AI Bias and Why Should We Care?

As AI systems become more deeply embedded in decisions ranging from credit approvals, hiring, and healthcare to criminal justice, public scrutiny of “unwanted” or “unfair” bias has intensified.

> *“We know that human decision-making in many areas is biased and shaped by our individual or societal biases… One may assume that using data to automate decisions would make everything fair, but we now know that this is not the case.”*

Even if an algorithm’s code is entirely neutral, the **data** that trains it can encode historical prejudices, incomplete samples, or labeling errors—thus perpetuating or magnifying injustices. The paper cites the infamous example of COMPAS recidivism prediction in Broward County, Florida, where African-American defendants were labeled “high-risk” at much higher rates than white defendants.

### The Challenge of Defining Fairness

> *“There are at least 21 mathematical definitions of fairness... different definitions focus on different aspects of fairness and thus can produce entirely different outcomes.”*

Fairness depends heavily on **context** (ethical, legal, cultural) and the **stakeholders’** perspectives. The paper discusses:

- **Individual Fairness:** “Treat similar individuals similarly.”  
- **Group Fairness:** “Ensure similar outcomes across groups (e.g., race, gender).”  
- **WAE (“We’re All Equal”)** vs. **WYSIWYG (“What You See Is What You Get”)**:  
  - Under WAE, we assume that statistical differences in the data reflect *societal* or *historical* bias and that groups should be treated as if they truly have no differences in ability.  
  - Under WYSIWYG, if the data shows differences in outcomes (e.g., test scores) between groups, the model uses those differences “as is.”  

### Where Does Bias Come From in Machine Learning?

Although the term “algorithmic bias” is widely used, the paper emphasizes that **data** is often the culprit:

> *“…the underlying data, rather than the algorithm, is the main source of bias… The biggest problem with machine learning models is that the training distribution does not always match the desired distribution.”*

Common data bias patterns:

- **Sample bias:** Over- or under-representation of a group.  
- **Label bias:** Data labels reflect human subjectivity or prejudice.  
- **Outcome proxy bias:** The target variable is an imperfect measure of what we *really* want. (For instance, using *arrests* as a proxy for *crime* is inherently skewed by where police choose to patrol.)

---

## 3. Core Concepts and Terminology

### Protected Attributes, Privileged vs. Unprivileged

A **protected attribute** is a feature such as race, gender, age, religion, disability, or sexual orientation that often serves as the basis for anti-discrimination laws or ethical concerns.  

- A *privileged* value is one that historically held systemic advantage (e.g., “white” in the U.S. context).  
- An *unprivileged* value is one that historically faced disadvantage or discrimination (e.g., “Black” or “Hispanic”).  

### Individual vs. Group Fairness

- **Individual Fairness** might say: “Two customers with nearly identical incomes, job statuses, and credit histories should receive similar outcomes.”  
- **Group Fairness** focuses on ensuring certain statistics (like acceptance rates) are similar across demographic groups.

### WAE vs. WYSIWYG

- **WAE**: We assume all groups actually have *intrinsically equal* abilities and that different outcomes in the data reflect biases.  
- **WYSIWYG**: We assume that if the data shows group differences (e.g., in test scores), then these differences are “real” or “usable” for our model.

### Key Fairness Metrics

There are many ways to measure unfairness. You might:

- **Compare acceptance rates**: e.g. “Do unprivileged groups have the same acceptance rate as privileged ones?”  
- **Compare error rates**: e.g. “Are false positives or false negatives distributed equally across groups?”  
- **Assess local individual fairness**: e.g. “Do two nearly identical individuals receive the same label?”

Because the paper notes *there is no single ‘best’ metric*, AIF360 includes **77** fairness metrics that can be computed at different stages of the ML pipeline, broken down into classes like:

1. **`DatasetMetric`** (e.g., for raw data)  
2. **`ClassificationMetric`** (e.g., for models’ predictions)  
3. **`SampleDistortionMetric`** (e.g., for transformations measuring how “far” the data was changed)

---

## 4. AIF360: Measuring Bias

### Overview of AIF360 and Its Goals

The paper introduces the open source [**AI Fairness 360 (AIF360)**](https://github.com/IBM/AIF360) toolkit, which:

> *“…combines the top bias metrics, bias mitigation algorithms, and metric explainers from fairness researchers across industry and academia.”*

AIF360 aims to:

- **Centralize** many known metrics and algorithms so they can be used in a consistent way (similar to scikit-learn’s fit/transform/predict).
- **Promote** a deeper understanding of fairness by providing metric “explainers.”
- **Enable** collaboration and accelerate fairness research in an industrial context.

### Classes of Fairness Metrics

AIF360’s **API** documentation categorizes the metrics into:

- **`BinaryLabelDatasetMetric`**: Measures bias in datasets with binary (favorable/unfavorable) labels.  
- **`ClassificationMetric`**: Measures bias in model predictions.  
- **`SampleDistortionMetric`**: Measures the distortion cost from transforming a dataset to reduce bias.

### Which Metrics Should You Use?

This is heavily use-case dependent. For example:

- **If you want to achieve equal acceptance rates** across groups (a WAE perspective), you might track **demographic parity** (a.k.a. *disparate impact*).  
- **If you aim for equal error rates** across groups (closer to WYSIWYG), you might track **equalized odds** or **equal opportunity** measures, such as *false positive rate difference* or *false negative rate difference*.  

As the paper notes:

> *“…there is no one best metric for every case. It is recommended to use several metrics… chosen with the guidance of subject matter experts and key stakeholders.”*

---

## 5. AIF360: Mitigating Bias

Beyond measuring bias, AIF360 provides multiple **mitigation** algorithms that can be applied at different stages of your ML pipeline:

1. **Pre-Processing** (transform the training data)  
2. **In-Processing** (modify the learning algorithm itself)  
3. **Post-Processing** (adjust final predictions)

### Pre-Processing

> *“With pre-processing algorithms, you attempt to reduce bias by manipulating the training data before training the algorithm.”*

Examples:

- **Reweighing**: Instead of editing feature values, you assign different sample *weights* to favor underrepresented groups.  
- **Optimized Pre-Processing**: Learns a probabilistic transformation to “correct” attributes and labels.  
- **Disparate-Impact Remover**: Edits feature values to preserve relative ordering within a group while improving demographic parity.

### In-Processing

If you have access to (and can modify) the learning procedure, you can incorporate *fairness constraints* directly into the training. Examples:

- **Adversarial Debiasing**: Uses an adversary network that tries to predict protected attributes from the model’s output; the main model is trained to confuse the adversary.  
- **Prejudice Remover**: Adds a “fairness penalty” into the objective function.  
- **Meta Fair Classifier**: Uses your chosen fairness metric as a direct training objective.

### Post-Processing

> *“…if you need to treat the learned model as a black box… you will need to use the post-processing algorithms.”*

Examples:

- **Calibrated Equalized Odds**: Adjusts the predicted labels (accept/reject) with specific probabilities to match a target error rate or acceptance rate across groups.  
- **Reject Option Classification**: Gives *favorable* outcomes to unprivileged groups and *unfavorable* outcomes to privileged groups in a zone of prediction uncertainty.

### Continuous Monitoring

Because real-world data distributions drift over time, the paper suggests integrating fairness checks into an **ongoing pipeline**, much like you do with ordinary functional or regression testing:

> *“…we recommend integrating continuous bias detection into your automated pipeline. AIF360 is compatible with the end-to-end machine learning workflow...”*

---

## 6. Step-by-Step Code Tutorial

Below is a simplified end-to-end example (adapted from the paper’s Chapter 3 and from the [AIF360 repository](https://github.com/IBM/AIF360)) that demonstrates how to:

1. **Load a dataset** with a protected attribute.  
2. **Measure** how biased it is (using a fairness metric).  
3. **Apply** a pre-processing bias mitigation algorithm.  
4. **Measure again** to see improvement.

This example uses the [**Adult Census dataset**](https://archive.ics.uci.edu/ml/datasets/adult), where the prediction task is: “Will a person’s income exceed \$50,000/year?” We’ll treat *race* as the protected attribute—specifically, individuals labeled as “white” vs. “not white.”

> **Note:** The code below largely follows the structure outlined in the paper, but you can find more extended demos in the official AIF360 examples directory.

### 6.1. Python Environment Setup

Install AIF360 in your Python environment:

```bash
pip install aif360
```

You will also need `numpy`, `scikit-learn`, and any plotting libraries you prefer (`matplotlib`, etc.) for more advanced workflows.

### 6.2. Import Statements

```python
import sys
sys.path.append("../")  # if you cloned AIF360 locally and want the examples

import numpy as np

# AIF360 imports
from aif360.metrics import BinaryLabelDatasetMetric
from aif360.algorithms.preprocessing.optim_preproc import OptimPreproc
from aif360.algorithms.preprocessing.optim_preproc_helpers \
    .data_preproc_functions import load_preproc_data_adult
from aif360.algorithms.preprocessing.optim_preproc_helpers \
    .distortion_functions import get_distortion_adult
from aif360.algorithms.preprocessing.optim_preproc_helpers \
    .opt_tools import OptTools

# For display formatting
from IPython.display import Markdown, display

np.random.seed(1)  # for reproducibility
```

### 6.3. Load the Dataset and Split into Train/Test

We load a preprocessed version of the *Adult* data from the AIF360 helper function. It automatically encodes attributes (like race) and labels them as favorable/unfavorable. We specify that `"race"` is our protected attribute of interest.

```python
# Load dataset where 'race' is the protected attribute
dataset_orig = load_preproc_data_adult(['race'])

# Split into training/test sets (70%/30%)
dataset_orig_train, dataset_orig_test = dataset_orig.split([0.7], shuffle=True)

# Define which groups are privileged vs. unprivileged for 'race'
privileged_groups = [{'race': 1}]  # e.g., 'race' = white
unprivileged_groups = [{'race': 0}]  # e.g., 'race' ≠ white
```

### 6.4. Compute Fairness Metric on the Original Training Dataset

Let’s see if the original training data is already “unfair.” We’ll measure the **difference in mean outcomes** (the acceptance rate or “favorable label” rate) for unprivileged vs. privileged groups, i.e. `unprivileged_rate - privileged_rate`.

```python
metric_orig_train = BinaryLabelDatasetMetric(dataset_orig_train,
    unprivileged_groups=unprivileged_groups,
    privileged_groups=privileged_groups)

display(Markdown("#### Original training dataset"))

diff = metric_orig_train.mean_difference()
print("Difference in mean outcomes (unpriv. - priv.) = %f" % diff)
```

A negative value indicates the unprivileged group is receiving fewer positive outcomes. For example:

```
#### Original training dataset
Difference in mean outcomes (unpriv. - priv.) = -0.104553
```

This means that, in the training data, the unprivileged group is getting a ~10.5 percentage point lower chance of a favorable outcome compared to the privileged group.

### 6.5. Mitigate Bias by Transforming the Training Dataset

To reduce this disparity, we apply the **Optimized Pre-Processing** algorithm:

```python
# Set tuning parameters for the optimization
optim_options = {
    "distortion_fun": get_distortion_adult,
    "epsilon": 0.05,
    "clist": [0.99, 1.99, 2.99],
    "dlist": [0.1, 0.05, 0]
}

OP = OptimPreproc(OptTools, optim_options)
OP = OP.fit(dataset_orig_train)

# Transform (or 'repair') the training dataset
dataset_transf_train = OP.transform(dataset_orig_train, transform_Y=True)

# Align features for easier comparison 
dataset_transf_train = dataset_orig_train.align_datasets(dataset_transf_train)
```

> *“…transforming the dataset can be extremely effective, but it introduces its own challenges: legal constraints on modifying data, potential loss of interpretability, etc.”*

### 6.6. Measure Bias on the Transformed Dataset

We measure the **mean difference** again on the new *transformed* dataset:

```python
metric_transf_train = BinaryLabelDatasetMetric(
    dataset_transf_train,
    unprivileged_groups=unprivileged_groups,
    privileged_groups=privileged_groups
)

display(Markdown("#### Transformed training dataset"))

diff_transf = metric_transf_train.mean_difference()
print("Difference in mean outcomes (unpriv. - priv.) = %f" % diff_transf)
```

Sample output might be:

```
#### Transformed training dataset
Difference in mean outcomes (unpriv. - priv.) = -0.051074
```

This shows the difference in average outcomes has improved from ~-0.105 to ~-0.051, a substantial reduction in bias against the unprivileged group.

---

## 7. Conclusion & Future Directions

### Final Takeaways

- **Bias is not limited** to harmful or illegal discrimination. Organizations may decide on fairness goals for strategic or ethical reasons even outside regulated protected attributes.  
- **Fairness requires** a holistic approach that includes both technical and organizational components. The best metrics and mitigation strategies differ by application.  
- **AIF360** offers a wide range of *metrics* and *algorithms* to help you identify, measure, and reduce unwanted bias.  
- **No single approach** to fairness mitigation can solve everything. Real-world solutions often combine pre-processing, in-processing, and/or post-processing, while continuously monitoring for drift.

> *“…fairness is a multifaceted, context-dependent social construct that defies simple definition… The metrics and algorithms in AIF 360… do not capture the full scope of fairness in all situations.”*

### The Future of Fairness in AI

The paper emphasizes:

> *“Many experts are now saying that unwanted bias might be the major barrier that prevents AI from reaching its full potential.”*

Addressing AI bias is a **multidisciplinary** challenge—requiring legal, ethical, and domain experts—alongside data scientists, designers, and engineers. As the authors note, future work will:

- Expand the scope of fairness algorithms.  
- Improve interpretability and transparency (e.g., *AI FactSheets*).  
- Encourage continuous, automated testing for fairness.

By integrating fairness best practices early and often, we can foster trust in AI systems that truly serve *all* parts of society.

---

## Additional References

- [IBM AI Fairness 360 GitHub Repository](https://github.com/IBM/AIF360)  
- [AIF360 Documentation & Tutorials](https://aif360.readthedocs.io/en/latest/)  
- [UCI Adult Dataset](https://archive.ics.uci.edu/ml/datasets/adult)  

---

### Quick Recap 

1. **AI models sometimes reflect real-life bias** because they learn from data that has biases.  
2. **Defining fairness is tricky**; we can’t fix what we can’t define or measure, so picking the right metric is crucial.  
3. **AIF360** is a helpful toolkit that can **measure bias** in your data or model outputs using many fairness metrics.  
4. **You can mitigate (reduce) bias** in different ways:
   - **Before** training (pre-processing).  
   - **While** training (in-processing).  
   - **After** training (post-processing).  
5. **It’s best to keep checking** for bias whenever your data changes—just like you do for accuracy, performance, or other metrics.  

By following these steps and tools, we make our AI **fairer, more transparent, and more trustworthy**—which is good for people, good for businesses, and good for society.
