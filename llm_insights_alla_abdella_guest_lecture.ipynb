{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3eLabQ16w2i3"
      },
      "source": [
        "## LLM Insights - Guest Lecture by Dr. Alla Abdella\n",
        "Welcome to this Jupyter Notebook demonstrating embeddings, retrieval with RAG, and usage of LLM-based pipelines with LangChain. We'll explore how to set up local LLMs, build a vector store, and create multi-step pipelines with memory and prompt templates."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EXPLANATION_1"
      },
      "source": [
        "# Embeddings & Retrieval: A Hands-On LangChain Demo"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install Core LangChain and Related Libraries\n",
        "!pip install -U langchain langchainhub langchain-nomic langchain_community langchain-groq tiktoken chromadb langgraph\n",
        "\n",
        "# Install Sentence Embedding Models\n",
        "!pip install sentence-transformers\n",
        "\n",
        "# Install LLM Libraries\n",
        "!pip install transformers gpt4all anthropic\n",
        "\n",
        "# Install Additional Data Processing and Visualization Libraries\n",
        "!pip install pandas scikit-learn matplotlib plotly\n",
        "\n",
        "# Install Streamlit for Web Apps\n",
        "!pip install streamlit\n",
        "\n",
        "# Install Tavily for API Integrations\n",
        "!pip install tavily-python\n",
        "\n",
        "# Upgrade Specific Tools and Libraries\n",
        "!pip install --upgrade langchain\n",
        "!pip install --upgrade --quiet langchain-text-splitters\n",
        "!pip install llama-index\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "c63ys5C5Jtjg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5c0a6025-457f-4567-d740-a3d954c75af8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.16)\n",
            "Collecting langchain\n",
            "  Downloading langchain-0.3.17-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting langchainhub\n",
            "  Downloading langchainhub-0.1.21-py3-none-any.whl.metadata (659 bytes)\n",
            "Collecting langchain-nomic\n",
            "  Downloading langchain_nomic-0.1.4-py3-none-any.whl.metadata (1.6 kB)\n",
            "Collecting langchain_community\n",
            "  Downloading langchain_community-0.3.16-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting langchain-groq\n",
            "  Downloading langchain_groq-0.2.4-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.8.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Collecting chromadb\n",
            "  Downloading chromadb-0.6.3-py3-none-any.whl.metadata (6.8 kB)\n",
            "Collecting langgraph\n",
            "  Downloading langgraph-0.2.69-py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.37)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (3.11.11)\n",
            "Collecting langchain-core<0.4.0,>=0.3.33 (from langchain)\n",
            "  Downloading langchain_core-0.3.33-py3-none-any.whl.metadata (6.3 kB)\n",
            "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.5)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.2)\n",
            "Requirement already satisfied: numpy<2,>=1.22.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (1.26.4)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.10.6)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain) (9.0.0)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchainhub) (24.2)\n",
            "Collecting types-requests<3.0.0.0,>=2.31.0.2 (from langchainhub)\n",
            "  Downloading types_requests-2.32.0.20241016-py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting nomic<4.0.0,>=3.1.2 (from langchain-nomic)\n",
            "  Downloading nomic-3.4.1.tar.gz (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.5/49.5 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pillow<11.0.0,>=10.3.0 (from langchain-nomic)\n",
            "  Downloading pillow-10.4.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (9.2 kB)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain_community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting httpx-sse<0.5.0,>=0.4.0 (from langchain_community)\n",
            "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain_community)\n",
            "  Downloading pydantic_settings-2.7.1-py3-none-any.whl.metadata (3.5 kB)\n",
            "Collecting groq<1,>=0.4.1 (from langchain-groq)\n",
            "  Downloading groq-0.16.0-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2024.11.6)\n",
            "Collecting build>=1.0.3 (from chromadb)\n",
            "  Downloading build-1.2.2.post1-py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting chroma-hnswlib==0.7.6 (from chromadb)\n",
            "  Downloading chroma_hnswlib-0.7.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (252 bytes)\n",
            "Collecting fastapi>=0.95.2 (from chromadb)\n",
            "  Downloading fastapi-0.115.8-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting uvicorn>=0.18.3 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading uvicorn-0.34.0-py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting posthog>=2.4.0 (from chromadb)\n",
            "  Downloading posthog-3.11.0-py2.py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (4.12.2)\n",
            "Collecting onnxruntime>=1.14.1 (from chromadb)\n",
            "  Downloading onnxruntime-1.20.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.5 kB)\n",
            "Requirement already satisfied: opentelemetry-api>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.16.0)\n",
            "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.29.0-py3-none-any.whl.metadata (2.2 kB)\n",
            "Collecting opentelemetry-instrumentation-fastapi>=0.41b0 (from chromadb)\n",
            "  Downloading opentelemetry_instrumentation_fastapi-0.50b0-py3-none-any.whl.metadata (2.1 kB)\n",
            "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.16.0)\n",
            "Requirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.21.0)\n",
            "Collecting pypika>=0.48.9 (from chromadb)\n",
            "  Downloading PyPika-0.48.9.tar.gz (67 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (4.67.1)\n",
            "Collecting overrides>=7.3.1 (from chromadb)\n",
            "  Downloading overrides-7.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.11/dist-packages (from chromadb) (6.5.2)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.70.0)\n",
            "Collecting bcrypt>=4.0.1 (from chromadb)\n",
            "  Downloading bcrypt-4.2.1-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (9.8 kB)\n",
            "Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.15.1)\n",
            "Collecting kubernetes>=28.1.0 (from chromadb)\n",
            "  Downloading kubernetes-32.0.0-py2.py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting mmh3>=4.0.1 (from chromadb)\n",
            "  Downloading mmh3-5.1.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (16 kB)\n",
            "Requirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.11/dist-packages (from chromadb) (3.10.15)\n",
            "Requirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.28.1)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (13.9.4)\n",
            "Collecting langgraph-checkpoint<3.0.0,>=2.0.10 (from langgraph)\n",
            "  Downloading langgraph_checkpoint-2.0.10-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting langgraph-sdk<0.2.0,>=0.1.42 (from langgraph)\n",
            "  Downloading langgraph_sdk-0.1.51-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.18.3)\n",
            "Collecting pyproject_hooks (from build>=1.0.3->chromadb)\n",
            "  Downloading pyproject_hooks-1.2.0-py3-none-any.whl.metadata (1.3 kB)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading marshmallow-3.26.0-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting starlette<0.46.0,>=0.40.0 (from fastapi>=0.95.2->chromadb)\n",
            "  Downloading starlette-0.45.3-py3-none-any.whl.metadata (6.3 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from groq<1,>=0.4.1->langchain-groq) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from groq<1,>=0.4.1->langchain-groq) (1.9.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from groq<1,>=0.4.1->langchain-groq) (1.3.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (2024.12.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (1.0.7)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (3.10)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.14.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.8.2)\n",
            "Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.27.0)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (1.3.1)\n",
            "Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (3.2.2)\n",
            "Requirement already satisfied: urllib3>=1.24.2 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.3.0)\n",
            "Requirement already satisfied: importlib-metadata<=8.6.1 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.33->langchain) (8.6.1)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.33->langchain) (1.33)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from langgraph-checkpoint<3.0.0,>=2.0.10->langgraph) (1.1.0)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nomic<4.0.0,>=3.1.2->langchain-nomic) (8.1.8)\n",
            "Collecting jsonlines (from nomic<4.0.0,>=3.1.2->langchain-nomic)\n",
            "  Downloading jsonlines-4.0.0-py3-none-any.whl.metadata (1.6 kB)\n",
            "Collecting loguru (from nomic<4.0.0,>=3.1.2->langchain-nomic)\n",
            "  Downloading loguru-0.7.3-py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from nomic<4.0.0,>=3.1.2->langchain-nomic) (2.2.2)\n",
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.11/dist-packages (from nomic<4.0.0,>=3.1.2->langchain-nomic) (17.0.0)\n",
            "Requirement already satisfied: pyjwt in /usr/local/lib/python3.11/dist-packages (from nomic<4.0.0,>=3.1.2->langchain-nomic) (2.10.1)\n",
            "Collecting coloredlogs (from onnxruntime>=1.14.1->chromadb)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (25.1.24)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (4.25.6)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (1.13.1)\n",
            "Requirement already satisfied: deprecated>=1.2.6 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (1.2.18)\n",
            "Requirement already satisfied: setuptools>=16.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (75.1.0)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.66.0)\n",
            "Collecting opentelemetry-exporter-otlp-proto-common==1.29.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_common-1.29.0-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting opentelemetry-proto==1.29.0 (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb)\n",
            "  Downloading opentelemetry_proto-1.29.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting opentelemetry-sdk>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_sdk-1.29.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting protobuf (from onnxruntime>=1.14.1->chromadb)\n",
            "  Downloading protobuf-5.29.3-cp38-abi3-manylinux2014_x86_64.whl.metadata (592 bytes)\n",
            "Collecting opentelemetry-instrumentation-asgi==0.50b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading opentelemetry_instrumentation_asgi-0.50b0-py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting opentelemetry-instrumentation==0.50b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading opentelemetry_instrumentation-0.50b0-py3-none-any.whl.metadata (6.1 kB)\n",
            "Collecting opentelemetry-semantic-conventions==0.50b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading opentelemetry_semantic_conventions-0.50b0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting opentelemetry-util-http==0.50b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading opentelemetry_util_http-0.50b0-py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-instrumentation==0.50b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (1.17.2)\n",
            "Collecting asgiref~=3.0 (from opentelemetry-instrumentation-asgi==0.50b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n",
            "  Downloading asgiref-3.8.1-py3-none-any.whl.metadata (9.3 kB)\n",
            "Collecting opentelemetry-api>=1.2.0 (from chromadb)\n",
            "  Downloading opentelemetry_api-1.29.0-py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting importlib-metadata<=8.5.0,>=6.0 (from opentelemetry-api>=1.2.0->chromadb)\n",
            "  Downloading importlib_metadata-8.5.0-py3-none-any.whl.metadata (4.8 kB)\n",
            "Collecting monotonic>=1.5 (from posthog>=2.4.0->chromadb)\n",
            "  Downloading monotonic-1.6-py2.py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting backoff>=1.10.0 (from posthog>=2.4.0->chromadb)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.27.2)\n",
            "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain_community)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->chromadb) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->chromadb) (2.18.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.11/dist-packages (from tokenizers>=0.13.2->chromadb) (0.27.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.9.0->chromadb) (1.5.4)\n",
            "Collecting httptools>=0.6.3 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading httptools-0.6.4-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
            "Collecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading uvloop-0.21.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Collecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb)\n",
            "  Downloading watchfiles-1.0.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (14.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.5.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.17.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2024.10.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata<=8.6.1->langchain-core<0.4.0,>=0.3.33->langchain) (3.21.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.33->langchain) (3.0.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.2)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->nomic<4.0.0,>=3.1.2->langchain-nomic) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->nomic<4.0.0,>=3.1.2->langchain-nomic) (2025.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.1)\n",
            "Downloading langchain-0.3.17-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m46.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchainhub-0.1.21-py3-none-any.whl (5.2 kB)\n",
            "Downloading langchain_nomic-0.1.4-py3-none-any.whl (3.9 kB)\n",
            "Downloading langchain_community-0.3.16-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m59.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_groq-0.2.4-py3-none-any.whl (14 kB)\n",
            "Downloading tiktoken-0.8.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m53.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading chromadb-0.6.3-py3-none-any.whl (611 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m611.1/611.1 kB\u001b[0m \u001b[31m29.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading chroma_hnswlib-0.7.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m60.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langgraph-0.2.69-py3-none-any.whl (148 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m148.7/148.7 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bcrypt-4.2.1-cp39-abi3-manylinux_2_28_x86_64.whl (278 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.6/278.6 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading build-1.2.2.post1-py3-none-any.whl (22 kB)\n",
            "Downloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading fastapi-0.115.8-py3-none-any.whl (94 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.8/94.8 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading groq-0.16.0-py3-none-any.whl (109 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109.7/109.7 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
            "Downloading kubernetes-32.0.0-py2.py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m57.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_core-0.3.33-py3-none-any.whl (412 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m412.7/412.7 kB\u001b[0m \u001b[31m27.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langgraph_checkpoint-2.0.10-py3-none-any.whl (37 kB)\n",
            "Downloading langgraph_sdk-0.1.51-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.7/44.7 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mmh3-5.1.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (101 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.6/101.6 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnxruntime-1.20.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (13.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m52.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_exporter_otlp_proto_grpc-1.29.0-py3-none-any.whl (18 kB)\n",
            "Downloading opentelemetry_exporter_otlp_proto_common-1.29.0-py3-none-any.whl (18 kB)\n",
            "Downloading opentelemetry_proto-1.29.0-py3-none-any.whl (55 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.8/55.8 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_instrumentation_fastapi-0.50b0-py3-none-any.whl (12 kB)\n",
            "Downloading opentelemetry_instrumentation-0.50b0-py3-none-any.whl (30 kB)\n",
            "Downloading opentelemetry_instrumentation_asgi-0.50b0-py3-none-any.whl (16 kB)\n",
            "Downloading opentelemetry_semantic_conventions-0.50b0-py3-none-any.whl (166 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.6/166.6 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_api-1.29.0-py3-none-any.whl (64 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.3/64.3 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_util_http-0.50b0-py3-none-any.whl (6.9 kB)\n",
            "Downloading opentelemetry_sdk-1.29.0-py3-none-any.whl (118 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.1/118.1 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading overrides-7.7.0-py3-none-any.whl (17 kB)\n",
            "Downloading pillow-10.4.0-cp311-cp311-manylinux_2_28_x86_64.whl (4.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m62.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading posthog-3.11.0-py2.py3-none-any.whl (72 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic_settings-2.7.1-py3-none-any.whl (29 kB)\n",
            "Downloading types-requests-2.32.0.20241016-py3-none-any.whl (15 kB)\n",
            "Downloading uvicorn-0.34.0-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading durationpy-0.9-py3-none-any.whl (3.5 kB)\n",
            "Downloading httptools-0.6.4-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (459 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m459.8/459.8 kB\u001b[0m \u001b[31m27.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading importlib_metadata-8.5.0-py3-none-any.whl (26 kB)\n",
            "Downloading marshmallow-3.26.0-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.8/50.8 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
            "Downloading protobuf-5.29.3-cp38-abi3-manylinux2014_x86_64.whl (319 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.7/319.7 kB\u001b[0m \u001b[31m26.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Downloading starlette-0.45.3-py3-none-any.whl (71 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading uvloop-0.21.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m53.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading watchfiles-1.0.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (452 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m452.6/452.6 kB\u001b[0m \u001b[31m24.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jsonlines-4.0.0-py3-none-any.whl (8.7 kB)\n",
            "Downloading loguru-0.7.3-py3-none-any.whl (61 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.6/61.6 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyproject_hooks-1.2.0-py3-none-any.whl (10 kB)\n",
            "Downloading asgiref-3.8.1-py3-none-any.whl (23 kB)\n",
            "Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Building wheels for collected packages: nomic, pypika\n",
            "  Building wheel for nomic (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nomic: filename=nomic-3.4.1-py3-none-any.whl size=49942 sha256=970ae5e16f74d7c88c7539a664ace2fb2813931deba983e4fdacb1c3398e1c27\n",
            "  Stored in directory: /root/.cache/pip/wheels/d3/c6/51/4a3cd698715ef6570c9311a5ec5bbf972d41d0c4f3d500e8e3\n",
            "  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pypika: filename=PyPika-0.48.9-py2.py3-none-any.whl size=53771 sha256=213e6574f44ed043bb7dde31ca1254e5ecd8b17acfe34b13d51f29f70acee957\n",
            "  Stored in directory: /root/.cache/pip/wheels/a3/01/bd/4c40ceb9d5354160cb186dcc153360f4ab7eb23e2b24daf96d\n",
            "Successfully built nomic pypika\n",
            "Installing collected packages: pypika, monotonic, durationpy, uvloop, uvicorn, types-requests, python-dotenv, pyproject_hooks, protobuf, pillow, overrides, opentelemetry-util-http, mypy-extensions, mmh3, marshmallow, loguru, jsonlines, importlib-metadata, humanfriendly, httpx-sse, httptools, chroma-hnswlib, bcrypt, backoff, asgiref, watchfiles, typing-inspect, tiktoken, starlette, posthog, opentelemetry-proto, opentelemetry-api, langchainhub, coloredlogs, build, pydantic-settings, opentelemetry-semantic-conventions, opentelemetry-exporter-otlp-proto-common, onnxruntime, nomic, langgraph-sdk, kubernetes, groq, fastapi, dataclasses-json, opentelemetry-sdk, opentelemetry-instrumentation, langchain-core, opentelemetry-instrumentation-asgi, opentelemetry-exporter-otlp-proto-grpc, langgraph-checkpoint, langchain-nomic, langchain-groq, opentelemetry-instrumentation-fastapi, langgraph, langchain, langchain_community, chromadb\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 4.25.6\n",
            "    Uninstalling protobuf-4.25.6:\n",
            "      Successfully uninstalled protobuf-4.25.6\n",
            "  Attempting uninstall: pillow\n",
            "    Found existing installation: pillow 11.1.0\n",
            "    Uninstalling pillow-11.1.0:\n",
            "      Successfully uninstalled pillow-11.1.0\n",
            "  Attempting uninstall: importlib-metadata\n",
            "    Found existing installation: importlib_metadata 8.6.1\n",
            "    Uninstalling importlib_metadata-8.6.1:\n",
            "      Successfully uninstalled importlib_metadata-8.6.1\n",
            "  Attempting uninstall: opentelemetry-api\n",
            "    Found existing installation: opentelemetry-api 1.16.0\n",
            "    Uninstalling opentelemetry-api-1.16.0:\n",
            "      Successfully uninstalled opentelemetry-api-1.16.0\n",
            "  Attempting uninstall: opentelemetry-semantic-conventions\n",
            "    Found existing installation: opentelemetry-semantic-conventions 0.37b0\n",
            "    Uninstalling opentelemetry-semantic-conventions-0.37b0:\n",
            "      Successfully uninstalled opentelemetry-semantic-conventions-0.37b0\n",
            "  Attempting uninstall: opentelemetry-sdk\n",
            "    Found existing installation: opentelemetry-sdk 1.16.0\n",
            "    Uninstalling opentelemetry-sdk-1.16.0:\n",
            "      Successfully uninstalled opentelemetry-sdk-1.16.0\n",
            "  Attempting uninstall: langchain-core\n",
            "    Found existing installation: langchain-core 0.3.32\n",
            "    Uninstalling langchain-core-0.3.32:\n",
            "      Successfully uninstalled langchain-core-0.3.32\n",
            "  Attempting uninstall: langchain\n",
            "    Found existing installation: langchain 0.3.16\n",
            "    Uninstalling langchain-0.3.16:\n",
            "      Successfully uninstalled langchain-0.3.16\n",
            "Successfully installed asgiref-3.8.1 backoff-2.2.1 bcrypt-4.2.1 build-1.2.2.post1 chroma-hnswlib-0.7.6 chromadb-0.6.3 coloredlogs-15.0.1 dataclasses-json-0.6.7 durationpy-0.9 fastapi-0.115.8 groq-0.16.0 httptools-0.6.4 httpx-sse-0.4.0 humanfriendly-10.0 importlib-metadata-8.5.0 jsonlines-4.0.0 kubernetes-32.0.0 langchain-0.3.17 langchain-core-0.3.33 langchain-groq-0.2.4 langchain-nomic-0.1.4 langchain_community-0.3.16 langchainhub-0.1.21 langgraph-0.2.69 langgraph-checkpoint-2.0.10 langgraph-sdk-0.1.51 loguru-0.7.3 marshmallow-3.26.0 mmh3-5.1.0 monotonic-1.6 mypy-extensions-1.0.0 nomic-3.4.1 onnxruntime-1.20.1 opentelemetry-api-1.29.0 opentelemetry-exporter-otlp-proto-common-1.29.0 opentelemetry-exporter-otlp-proto-grpc-1.29.0 opentelemetry-instrumentation-0.50b0 opentelemetry-instrumentation-asgi-0.50b0 opentelemetry-instrumentation-fastapi-0.50b0 opentelemetry-proto-1.29.0 opentelemetry-sdk-1.29.0 opentelemetry-semantic-conventions-0.50b0 opentelemetry-util-http-0.50b0 overrides-7.7.0 pillow-10.4.0 posthog-3.11.0 protobuf-5.29.3 pydantic-settings-2.7.1 pypika-0.48.9 pyproject_hooks-1.2.0 python-dotenv-1.0.1 starlette-0.45.3 tiktoken-0.8.0 types-requests-2.32.0.20241016 typing-inspect-0.9.0 uvicorn-0.34.0 uvloop-0.21.0 watchfiles-1.0.4\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL",
                  "importlib_metadata"
                ]
              },
              "id": "eccd667d50f84f0090a23dcf1f879d53"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (3.3.1)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.47.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.5.1+cu124)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (0.27.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (10.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (3.17.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.5)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (1.26.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2024.12.14)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m110.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m69.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m46.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m77.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~vidia-cublas-cu12 (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~vidia-cublas-cu12 (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.47.1)\n",
            "Collecting gpt4all\n",
            "  Downloading gpt4all-2.8.2-py3-none-manylinux1_x86_64.whl.metadata (4.8 kB)\n",
            "Collecting anthropic\n",
            "  Downloading anthropic-0.45.2-py3-none-any.whl.metadata (23 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.17.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.27.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EXPLANATION_2"
      },
      "source": [
        "**Explanation of Installed Libraries:**  \n",
        "- **langchain, langchainhub, etc.**: Libraries that help in chaining together different steps of a Language Model pipeline—like retrieving documents, running prompts, storing conversation memory, etc.\n",
        "- **sentence-transformers**: Allows us to convert sentences into vectors (fancy math shapes) so we can compare how similar they are.\n",
        "- **transformers, gpt4all, anthropic**: Different ways of using and working with advanced AI language models.\n",
        "- **pandas, scikit-learn, matplotlib, plotly**: Tools for data analysis and making pretty graphs.\n",
        "- **streamlit**: A library to build web apps easily.\n",
        "- **tavily-python**: Helps with certain API integrations.\n",
        "- **tiktoken**: A library from OpenAI to manage text tokens efficiently.\n",
        "- **llama-index**: Another library to assist with building large language model indexes.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import ConversationChain\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.chains import SequentialChain\n",
        "\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.chains import SimpleSequentialChain\n",
        "from langchain_groq import ChatGroq\n",
        "import bs4\n",
        "from langchain import hub\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_community.embeddings import GPT4AllEmbeddings\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "import os\n",
        "os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
        "os.environ['LANGCHAIN_ENDPOINT'] = 'https://api.smith.langchain.com'\n",
        "os.environ['LANGCHAIN_API_KEY'] = \"\"\n",
        "os.environ[\"GROQ_API_KEY\"] = \"\"\n",
        "os.environ[\"LANGSMITH_API_KEY\"] = \"\"\n",
        "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
        "os.environ[\"HF_TOKEN\"] = \"\""
      ],
      "metadata": {
        "id": "-M7YZPrZLN0m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EXPLANATION_3"
      },
      "source": [
        "**What this code does:**\n",
        "- **Imports** a variety of components from the LangChain ecosystem: chains, memory, prompts, vector stores, embeddings, etc.\n",
        "- **ChatGroq** is a specialized class from `langchain_groq` that integrates with a particular LLM service.\n",
        "- **Sets environment variables** so we don’t have to manually input keys or tokens when connecting to these services."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "\n",
        "# Initialize the SentenceTransformer model.\n",
        "model = SentenceTransformer(model_name)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "I16ErBrBKZsT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EXPLANATION_4"
      },
      "source": [
        "**Explanation:**  \n",
        "We set up a model named `all-MiniLM-L6-v2`, which is a well-known **Sentence Transformers** model. It is good for quickly generating embeddings (vectors) for short or medium-length sentences."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the model to encode a simple string.\n",
        "model.encode(\"Hello Students\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "0DQioC4cKDiR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's see the length of the vector.\n",
        "len(model.encode(\"Hello Students\"))"
      ],
      "metadata": {
        "id": "J6jmirRbKDoq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EXPLANATION_5"
      },
      "source": [
        "**What’s happening here?**  \n",
        "- We first **encode** the phrase \"Hello Students\" into a numerical vector.\n",
        "- Then we check the **length** of that vector.\n",
        "- This length depends on the dimension of the embedding that the model outputs (384 for `all-MiniLM-L6-v2`)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# 1. Initialize a pre-trained model\n",
        "# model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "# model = SentenceTransformer(model_name)\n",
        "\n",
        "# 2. Define sentences related to Urdu students and Pakistani culture\n",
        "sentences = [\n",
        "    # Greetings\n",
        "    \"Assalam-o-Alaikum! How are you today?\",\n",
        "    \"Good morning! I hope you are doing well.\",\n",
        "    \"Hi there! How has your day been?\",\n",
        "\n",
        "    # Technology\n",
        "    \"Learning Python is essential for students.\",\n",
        "    \"Programming in Python is very interesting.\",\n",
        "    \"Python is a useful language for students.\",\n",
        "\n",
        "    # Food\n",
        "    \"Biryani is a delicious dish from Pakistan.\",\n",
        "    \"I love eating spicy biryani with friends.\",\n",
        "    \"Pakistani biryani is the best comfort food.\",\n",
        "]\n",
        "\n",
        "# 3. Compute embeddings\n",
        "embeddings = model.encode(sentences)\n",
        "\n",
        "# 4. Apply PCA to reduce to 2D for visualization\n",
        "pca = PCA(n_components=2)\n",
        "reduced_embeddings = pca.fit_transform(embeddings)\n",
        "\n",
        "# 5. Plot the PCA results\n",
        "plt.figure(figsize=(12, 8))\n",
        "colors = ['red', 'green', 'blue']  # Unique color for each category\n",
        "categories = ['Greetings', 'Technology', 'Food']  # Labels for the legend\n",
        "\n",
        "# Define category boundaries\n",
        "category_size = 3  # Number of sentences per category\n",
        "num_categories = len(sentences) // category_size\n",
        "\n",
        "for i in range(num_categories):\n",
        "    start_idx = i * category_size\n",
        "    end_idx = start_idx + category_size\n",
        "    cluster = reduced_embeddings[start_idx:end_idx]\n",
        "    label_group = sentences[start_idx:end_idx]\n",
        "    color = colors[i % len(colors)]\n",
        "    plt.scatter(cluster[:, 0], cluster[:, 1], color=color, label=categories[i])  # Use category label\n",
        "\n",
        "    # Annotate each point\n",
        "    for j, txt in enumerate(label_group):\n",
        "        plt.annotate(txt, (cluster[j, 0], cluster[j, 1]), fontsize=9, alpha=0.7)\n",
        "\n",
        "plt.title(\"PCA of Sentence Embeddings\")\n",
        "plt.xlabel(\"PCA Component 1\")\n",
        "plt.ylabel(\"PCA Component 2\")\n",
        "plt.legend(title=\"Categories\")\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "4r6B_txXOXOh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EXPLANATION_6"
      },
      "source": [
        "**Step-by-step**:\n",
        "1. We have a list of sentences in different categories (Greetings, Technology, Food).\n",
        "2. We turn them into embeddings using our model.\n",
        "3. We then use **PCA** to reduce the dimension to 2D so we can plot them.\n",
        "4. We color them by category to see if sentences in similar topics end up close to each other in the chart."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"I love math\"\n",
        "\n",
        "# The tokens are the individual words or subwords.\n",
        "tokens =  [\"I\", \"love\", \"math\"]\n",
        "\n",
        "# This dictionary maps each token to its unique integer ID.\n",
        "tokens2id = {token: i for i, token in enumerate(tokens)}\n",
        "\n",
        "tokens2id\n"
      ],
      "metadata": {
        "id": "zZEOh5d_P_gq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EXPLANATION_7"
      },
      "source": [
        "**Explanation:**\n",
        "- This snippet demonstrates a simple example of how **tokenization** might work: splitting text into tokens, then assigning each token an ID.\n",
        "- In real NLP systems, tokenization can be more complex (handling punctuation, unknown words, etc.)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# #!pip install llama_index==0.10.18\n",
        "# \"\"\"\n",
        "# This is a simple application for sentence embeddings: semantic search\n",
        "#\n",
        "# We have a corpus with various sentences. Then, for a given query sentence,\n",
        "# we want to find the most similar sentence in this corpus.\n",
        "#\n",
        "# This script outputs for various queries the top 5 most similar sentences in the corpus.\n",
        "# \"\"\"\n",
        "\n",
        "# import torch\n",
        "# from sentence_transformers import SentenceTransformer\n",
        "# import torch\n",
        "# import numpy as np\n",
        "# import matplotlib.pyplot as plt\n",
        "# from mpl_toolkits.mplot3d import Axes3D  # Required for 3D plotting even if not referenced explicitly\n",
        "# from sklearn.decomposition import PCA\n",
        "# import plotly.graph_objects as go\n",
        "# # from llama_index.embeddings.fastembed import FastEmbedEmbedding\n",
        "\n",
        "# embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "# #embedder = FastEmbedEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n",
        "\n",
        "# corpus = [\n",
        "#     \"A man is eating food.\",\n",
        "#     \"A man is eating a piece of bread.\",\n",
        "#     \"The girl is carrying a baby.\",\n",
        "#     \"A man is riding a horse.\",\n",
        "#     \"A woman is playing violin.\",\n",
        "#     \"Two men pushed carts through the woods.\",\n",
        "#     \"A man is riding a white horse on an enclosed ground.\",\n",
        "#     \"A monkey is playing drums.\",\n",
        "#     \"A cheetah is running behind its prey.\",\n",
        "# ]\n",
        "# # corpus_embeddings = embedder.encode(corpus, convert_to_tensor=True)\n",
        "#\n",
        "# # queries = [\n",
        "# #     \"A man is eating pasta.\",\n",
        "# # ]\n",
        "#\n",
        "# # top_k = min(2, len(corpus))\n",
        "# # We'll store the query embeddings (as tensor) so that we can do PCA on all vectors together:\n",
        "# query_embeddings_list = []\n",
        "# results = {}  # To store top-2 indices for each query\n",
        "#\n",
        "# for query in queries:\n",
        "#     query_embedding = embedder.encode(query, convert_to_tensor=True)\n",
        "#     # similarity_scores = torch.matmul(query_embedding, corpus_embeddings.T)[0]\n",
        "#     # scores, indices = torch.topk(similarity_scores, k=top_k)\n",
        "#\n",
        "#     print(\"\\nQuery:\", query)\n",
        "#     print(\"Top 5 most similar sentences in corpus:\")\n",
        "#\n",
        "#     for score, idx in zip(scores, indices):\n",
        "#         print(corpus[idx], f\"(Score: {score:.4f})\")\n",
        "#\n",
        "#     query_embeddings_list.append(query_embedding[0].numpy())\n",
        "#     results[query] = {\n",
        "#         'top2_indices': indices[:2].numpy(),\n",
        "#         'similarity_scores': scores[:2].numpy()\n",
        "#     }\n",
        "#\n",
        "# # The rest of the code does 3D PCA plotting, omitted here.\n"
      ],
      "metadata": {
        "id": "XZSI21qFQjpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EXPLANATION_8"
      },
      "source": [
        "**Explanation:**\n",
        "This entire cell is commented out code. It’s a more extensive example of how to perform **semantic search** with embeddings:\n",
        "- Creating embeddings for a **corpus** of sentences.\n",
        "- Creating an embedding for a **query**.\n",
        "- Using **cosine similarity** (or dot product) to find which sentences in the corpus are closest to the query.\n",
        "- Retrieving the top `k` most similar sentences.\n",
        "- Optionally applying dimensionality reduction to plot them in 2D/3D."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install colab-xterm\n",
        "%load_ext colabxterm"
      ],
      "metadata": {
        "id": "Yr54wLLnKDsM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EXPLANATION_9"
      },
      "source": [
        "**What is `colab-xterm`?**  \n",
        "`colab-xterm` allows you to open a terminal-like interface directly in Google Colab. This can be handy for installing packages, exploring the file system, or running Linux commands in a more interactive shell."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%xterm\n",
        "# curl -fsSL https://ollama.com/install.sh | sh\n",
        "# ollama serve &\n",
        "# ollama run llama3.2:3b"
      ],
      "metadata": {
        "id": "6xswRAXRKDvC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EXPLANATION_10"
      },
      "source": [
        "**Explanation**:\n",
        "- The magic command `%xterm` will open an xterm in the Colab environment.\n",
        "- The commented lines show how one might install and run `ollama`, a local LLM server, though they’re optional."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import os\n",
        "import json\n",
        "\n",
        "### Index\n",
        "\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_community.embeddings import GPT4AllEmbeddings\n",
        "\n",
        "urls = [\n",
        "     \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n",
        "     \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\",\n",
        "     # You can add or remove URLs here.\n",
        "]\n",
        "\n",
        "# Load each URL with WebBaseLoader.\n",
        "docs = [WebBaseLoader(url).load() for url in urls]\n",
        "\n",
        "# Flatten the list of lists:\n",
        "docs_list = [item for sublist in docs for item in sublist]\n",
        "\n",
        "# Split the text into smaller chunks.\n",
        "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
        "    chunk_size=250,\n",
        "    chunk_overlap=50\n",
        ")\n",
        "doc_splits = text_splitter.split_documents(docs_list)\n",
        "\n",
        "# Build a Chroma vectorstore from the splitted documents.\n",
        "vectorstore = Chroma.from_documents(\n",
        "    documents=doc_splits,\n",
        "    collection_name=\"rag-chroma1\",\n",
        "    embedding=GPT4AllEmbeddings(),\n",
        ")\n",
        "\n",
        "# Convert vectorstore to a retriever.\n",
        "retriever = vectorstore.as_retriever(k=5)\n",
        "\n",
        "# Query and retrieve.\n",
        "query = \"What are the critical features of a gun\"\n",
        "documents = retriever.invoke(query)  # get relevant doc chunks\n",
        "\n",
        "# Construct the context from retrieved documents.\n",
        "context = \"\\n\\n\".join([f\"Document {i+1}:\\n{doc.page_content}\" for i, doc in enumerate(documents)])\n",
        "\n",
        "# RAG prompt.\n",
        "prompt = f\"\"\"\n",
        "You are an expert in Generative AI and autonomous agent systems. Below is the context retrieved from relevant documents. Use this context to provide a detailed and accurate answer to the user's query.\n",
        "If you don't know the answer, say you don't know.\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Query:\n",
        "{query}\n",
        "\n",
        "Answer:\n",
        "\"\"\"\n",
        "\n",
        "# Next, we call a local LLM API endpoint.\n",
        "url = 'http://localhost:11434/api/generate'\n",
        "payload = {\n",
        "    \"model\": \"llama3.2:3b\",\n",
        "    \"prompt\": prompt,\n",
        "    \"num_predict\": 2000,\n",
        "    \"temperature\": 0.0,\n",
        "    \"stream\": True\n",
        "}\n",
        "\n",
        "response = requests.post(url, json=payload, stream=True)\n",
        "\n",
        "if response.status_code == 200:\n",
        "    print(\"Model Response:\")\n",
        "    assembled_response = \"\"\n",
        "    for line in response.iter_lines(decode_unicode=True):\n",
        "        if line.strip():\n",
        "            try:\n",
        "                data = json.loads(line)\n",
        "                if \"response\" in data:\n",
        "                    assembled_response += data[\"response\"]\n",
        "                    print(data[\"response\"], end='', flush=True)\n",
        "                if data.get(\"done\", False):\n",
        "                    break\n",
        "            except json.JSONDecodeError as e:\n",
        "                print(f\"Error decoding JSON: {e}\")\n",
        "else:\n",
        "    print(f\"Error: {response.status_code} - {response.text}\")"
      ],
      "metadata": {
        "id": "BHNEaRxTZuYv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EXPLANATION_11"
      },
      "source": [
        "**Explanation**:\n",
        "1. **Load** web documents from the provided URLs.\n",
        "2. **Split** them up into smaller pieces with `RecursiveCharacterTextSplitter`.\n",
        "3. **Embed** these chunks using `GPT4AllEmbeddings` and store them in a `Chroma` vector database.\n",
        "4. Turn that database into a **retriever**.\n",
        "5. Issue a **query**, retrieve the most relevant chunks.\n",
        "6. Build a new prompt that includes those chunks as context.\n",
        "7. **POST** that prompt to a local LLM endpoint for the final answer."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "retriever.invoke(query)"
      ],
      "metadata": {
        "id": "6LChxSmEZzCE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EXPLANATION_12"
      },
      "source": [
        "**Explanation**:\n",
        "We simply call `retriever.invoke(query)` again to see the raw retrieved documents or chunks. This typically returns a list of documents best matching your query."
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sp6MHkf1ZzEy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EXPLANATION_13"
      },
      "source": [
        "Empty cell."
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "U0pBKSMsZzIY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EXPLANATION_14"
      },
      "source": [
        "Another empty cell."
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_KJ_fmcqZzK8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EXPLANATION_15"
      },
      "source": [
        "Again, empty cell."
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RyTx0rCeZzOP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EXPLANATION_16"
      },
      "source": [
        "One more empty code cell."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#export LANGCHAIN_TRACING_V2=true\n",
        "from langsmith import traceable\n",
        "from langchain_groq import ChatGroq\n",
        "import bs4\n",
        "from langchain import hub\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_community.embeddings import GPT4AllEmbeddings\n",
        "import os\n",
        "os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
        "os.environ['LANGCHAIN_ENDPOINT'] = 'https://api.smith.langchain.com'\n",
        "os.environ['LANGCHAIN_API_KEY'] = \"\"\n",
        "os.environ[\"GROQ_API_KEY\"] = \"\"\n",
        "os.environ[\"LANGSMITH_API_KEY\"] = \"\"\n",
        "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
        "os.environ[\"HF_TOKEN\"] = \"\"\n",
        "\n",
        "### LLM\n",
        "from langchain_groq import ChatGroq\n",
        "\n",
        "# Here we configure a ChatGroq instance.\n",
        "llm = ChatGroq(\n",
        "    model=\"llama-3.3-70b-versatile\",#\"mixtral-8x7b-32768\",\n",
        "    temperature=0,\n",
        "    max_tokens=None,\n",
        "    timeout=None,\n",
        "    max_retries=2,\n",
        ")\n",
        "\n",
        "chat = llm\n",
        "\n",
        "# Re-initialize the ChatGroq LLM.\n",
        "llm = ChatGroq(\n",
        "    model=\"llama-3.3-70b-versatile\",\n",
        "    temperature=0,\n",
        "    max_tokens=None,\n",
        "    timeout=None,\n",
        "    max_retries=2,\n",
        ")\n",
        "\n",
        "@traceable\n",
        "def get_messages():\n",
        "    messages = [\n",
        "        (\n",
        "            \"system\",\n",
        "            \"You are a helpful AI assistant with deep expertise in AI, data engineering, and healthcare.\"\n",
        "        ),\n",
        "        (\n",
        "            \"human\",\n",
        "            \"\"\" Please give me advices on how i start learing about the data science filed\n",
        "\n",
        "            Please provide markdown that demonstrates:\n",
        "              1. Use of syntax highlighting for different programming languages\n",
        "              2. Colored code blocks\n",
        "              3. Varied text styling (bold, italic, headers)\n",
        "              4. Include at least one example of colored terminal/output text\n",
        "            \"\"\"\n",
        "        ),\n",
        "    ]\n",
        "    return messages\n",
        "\n",
        "@traceable(run_type=\"llm\")\n",
        "def invoke_llm(messages):\n",
        "    return llm.invoke(messages)\n",
        "\n",
        "@traceable\n",
        "def parse_output(response):\n",
        "    return response.content\n",
        "\n",
        "@traceable\n",
        "def run_pipeline():\n",
        "    messages = get_messages()\n",
        "    response = invoke_llm(messages)\n",
        "    result = parse_output(response)\n",
        "    return result\n",
        "\n",
        "result = run_pipeline()\n",
        "\n",
        "from IPython.display import display, Markdown\n",
        "display(Markdown(result))"
      ],
      "metadata": {
        "collapsed": true,
        "id": "V4wlf3aqNoRn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EXPLANATION_17"
      },
      "source": [
        "**Explanation**:\n",
        "- Using `@traceable` from `langsmith` to track function calls.\n",
        "- `get_messages()` sets up system/human messages.\n",
        "- `invoke_llm(messages)` calls the LLM.\n",
        "- `parse_output(response)` returns the model output.\n",
        "- `run_pipeline()` orchestrates.\n",
        "- Finally, we display the result using `Markdown(result)`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RtCdKlj7PDW9"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Today's Agenda\n",
        "\n",
        "1. **Concepts of embeddings and embedding layers**  \n",
        "2. **Cost function(s) for training an embedding model**  \n",
        "3. **Practical demonstration in Python using Sentence Transformers**  \n",
        "4. **Sample training data in multiple formats**  \n",
        "5. **Location of the embedding layer in BERT-based models**  \n",
        "6. **Example code with multilingual sentences (English, Urdu, Arabic)**  \n",
        "7. **PCA visualization to show similar sentences clustering**  \n",
        "8. **Build a simple RAG (Retrieval-Augmented Generation)**  \n",
        "9. **Build Habib Conversational Agent**  \n",
        "10. **LangChain Chains**  \n",
        "11. **LangChain Memory**\n",
        "\n",
        "---\n",
        "\n",
        "## Slide 1: Introduction to Embeddings\n",
        "\n",
        "**Slide Content:**\n",
        "- **Definition**: An embedding is a dense vector representation of text (or tokens) in a continuous vector space.\n",
        "- **Key Idea**:\n",
        "  - Each dimension in the vector space captures latent semantic or syntactic information.\n",
        "- **Why embeddings?**\n",
        "  - They allow models to capture semantic similarity, context, and relationships between words/sentences.\n",
        "\n",
        "**Notes:**\n",
        "- They are the foundation of modern NLP tasks (retrieval, classification, similarity, etc.).\n",
        "\n",
        "---\n",
        "\n",
        "## Slide 2: What is an Embedding Layer?\n",
        "\n",
        "**Slide Content:**\n",
        "- **Embedding Layer**: A neural network layer that maps each token to a trainable dense vector.\n",
        "- **In BERT-like models**:\n",
        "  - The embedding layer is part of the initial model component.\n",
        "  - Input tokens are converted into embeddings, then fed into multiple attention layers.\n",
        "\n",
        "**Notes:**\n",
        "- In **BERT** and many transformer-based models:\n",
        "  1. **Token embeddings**  \n",
        "  2. **Position embeddings**  \n",
        "- The entire model (including the embedding layer) is typically trained end-to-end during fine-tuning.\n",
        "\n",
        "---\n",
        "\n",
        "## Slide 3: Cost Functions for Training Embeddings\n",
        "\n",
        "**Slide Content:**\n",
        "- **Objective**: Ensure that semantically similar sentences/words have closer embeddings, while dissimilar ones are farther apart.\n",
        "- Common **Loss Functions** in Sentence-Transformers:\n",
        "  1. **Triplet Loss**\n",
        "  2. **Contrastive Loss**\n",
        "  3. **Multiple Negative Ranking Loss**\n",
        "\n",
        "** Notes:**\n",
        "- The choice depends on how your data is formatted.\n",
        "- Triplet: (anchor, positive, negative).\n",
        "- Contrastive or CosineSimilarityLoss can use pairs with similarity scores.\n",
        "\n",
        "---\n",
        "\n",
        "## Slide 4: Sample Training Data Formats\n",
        "\n",
        "**Slide Content:**\n",
        "1. **Triplet Format** \\((anchor, positive, negative)\\)  \n",
        "2. **Constructive/Contrastive Pairs**  \n",
        "   \\((sentence_1, sentence_2, label)\\)  \n",
        "3. **Sentence Pairs with a Score**  \n",
        "   \\((sentence_1, sentence_2, score)\\)\n",
        "\n",
        "** Notes:**\n",
        "- Triplet Example: (\"How to prepare for board exams\", \"Tips to study effectively\", \"Best recipes for biryani\").\n",
        "- Pair with Score Example: (\"The cat sits on the mat\", \"A cat is lying on a rug\", 0.8).\n",
        "\n",
        "---\n",
        "\n",
        "## Slide 5: Where is the Embedding Layer in BERT?\n",
        "\n",
        "**Slide Content:**\n",
        "- **BERT Architecture** (simplified):\n",
        "  1. **Input Embeddings** (Token + Position )\n",
        "  2. **Transformer Layers**\n",
        "  3. **Pooler**\n",
        "- Embedding layer is at the very beginning.\n",
        "\n",
        "** Notes:**\n",
        "- Typically, we do **fine-tuning** on the entire BERT model.\n",
        "- **Sentence-Transformers** adds a pooling layer on top to create fixed-size sentence embeddings.\n",
        "\n",
        "---\n",
        "\n",
        "## Slide 6:  Detailed Python Example 1\n",
        "\n",
        "**Code Explanation**  \n",
        "1. Load a pre-trained sentence-transformer model.  \n",
        "2. Use sample sentences in English.\n",
        "3. Compute embeddings, reduce to 2D via PCA.\n",
        "4. Plot to illustrate that similar sentences cluster.\n",
        "\n",
        "```python\n",
        "!pip install sentence-transformers scikit-learn matplotlib --quiet\n",
        "\n",
        "\"\"\"\n",
        "Simple application for sentence embeddings: semantic search\n",
        "\n",
        "We have a corpus with various sentences. Then, for a given query sentence,\n",
        "we want to find the most similar sentence.\n",
        "We output the top 5 most similar.\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D  # 3D plotting\n",
        "from sklearn.decomposition import PCA\n",
        "import plotly.graph_objects as go\n",
        "from llama_index.embeddings.fastembed import FastEmbedEmbedding\n",
        "\n",
        "# embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "embedder = FastEmbedEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n",
        "\n",
        "corpus = [\n",
        "    \"A man is eating food.\",\n",
        "    \"A man is eating a piece of bread.\",\n",
        "    \"The girl is carrying a baby.\",\n",
        "    \"A man is riding a horse.\",\n",
        "    \"A woman is playing violin.\",\n",
        "    \"Two men pushed carts through the woods.\",\n",
        "    \"A man is riding a white horse on an enclosed ground.\",\n",
        "    \"A monkey is playing drums.\",\n",
        "    \"A cheetah is running behind its prey.\",\n",
        "]\n",
        "corpus_embeddings = embedder.get_text_embedding_batch(corpus, show_progress=True)\n",
        "\n",
        "if isinstance(corpus_embeddings, list):\n",
        "    corpus_embeddings = torch.tensor(corpus_embeddings)\n",
        "\n",
        "queries = [\n",
        "    \"A man is eating pasta.\",\n",
        "]\n",
        "\n",
        "top_k = min(2, len(corpus))\n",
        "query_embeddings_list = []\n",
        "results = {}\n",
        "\n",
        "for query in queries:\n",
        "    query_embedding = embedder.get_text_embedding_batch([query], show_progress=True)\n",
        "    if isinstance(query_embedding, list):\n",
        "        query_embedding = torch.tensor(query_embedding)\n",
        "\n",
        "    similarity_scores = torch.matmul(query_embedding, corpus_embeddings.T)[0]\n",
        "\n",
        "    scores, indices = torch.topk(similarity_scores, k=top_k)\n",
        "\n",
        "    print(\"\\nQuery:\", query)\n",
        "    print(\"Top 5 most similar sentences in corpus:\")\n",
        "\n",
        "    for score, idx in zip(scores, indices):\n",
        "        print(corpus[idx], f\"(Score: {score:.4f})\")\n",
        "\n",
        "    query_embeddings_list.append(query_embedding[0].numpy())\n",
        "    results[query] = {\n",
        "        'top2_indices': indices[:2].numpy(),\n",
        "        'similarity_scores': scores[:2].numpy()\n",
        "    }\n",
        "\n",
        "corpus_np = corpus_embeddings.cpu().detach().numpy()\n",
        "all_embeddings = np.vstack([corpus_np, np.array(query_embeddings_list)])\n",
        "\n",
        "pca = PCA(n_components=3)\n",
        "all_embeddings_3d = pca.fit_transform(all_embeddings)\n",
        "\n",
        "num_corpus = corpus_np.shape[0]\n",
        "corpus_3d = all_embeddings_3d[:num_corpus]\n",
        "queries_3d = all_embeddings_3d[num_corpus:]\n",
        "\n",
        "fig = plt.figure(figsize=(10, 8))\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "ax.scatter(corpus_3d[:, 0], corpus_3d[:, 1], corpus_3d[:, 2], c='blue', marker='o', s=80, label='Corpus')\n",
        "\n",
        "for i, txt in enumerate(corpus):\n",
        "    ax.text(corpus_3d[i, 0], corpus_3d[i, 1], corpus_3d[i, 2], txt, size=9, zorder=1, color='k')\n",
        "\n",
        "ax.scatter(queries_3d[:, 0], queries_3d[:, 1], queries_3d[:, 2], c='red', marker='^', s=120, label='Query')\n",
        "\n",
        "for i, query in enumerate(queries):\n",
        "    q_point = queries_3d[i]\n",
        "    top2_indices = results[query]['top2_indices']\n",
        "    for idx in top2_indices:\n",
        "        c_point = corpus_3d[idx]\n",
        "        ax.plot([q_point[0], c_point[0]], [q_point[1], c_point[1]], [q_point[2], c_point[2]], 'g--', linewidth=1.5)\n",
        "\n",
        "ax.set_xlabel(\"PCA Component 1\")\n",
        "ax.set_ylabel(\"PCA Component 2\")\n",
        "ax.set_zlabel(\"PCA Component 3\")\n",
        "ax.set_title(\"3D Visualization of Sentence Embeddings\\nLines connect each query to its 2 most similar corpus sentences\")\n",
        "ax.legend()\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "---\n",
        "## Slide 7:  Detailed Python Example 2\n",
        "\n",
        "```python\n",
        "# Install necessary libraries\n",
        "#!pip install sentence-transformers scikit-learn matplotlib plotly --quiet\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# 1. Initialize a pre-trained model\n",
        "model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "model = SentenceTransformer(model_name)\n",
        "\n",
        "# 2. Define sentences related to Urdu students and Pakistani culture\n",
        "sentences = [\n",
        "    # Greetings\n",
        "    \"Assalam-o-Alaikum! How are you today?\",\n",
        "    \"Good morning! I hope you are doing well.\",\n",
        "    \"Hi there! How has your day been?\",\n",
        "\n",
        "    # Technology\n",
        "    \"Learning Python is essential for students.\",\n",
        "    \"Programming in Python is very interesting.\",\n",
        "    \"Python is a useful language for students.\",\n",
        "\n",
        "    # Food\n",
        "    \"Biryani is a delicious dish from Pakistan.\",\n",
        "    \"I love eating spicy biryani with friends.\",\n",
        "    \"Pakistani biryani is the best comfort food.\",\n",
        "]\n",
        "\n",
        "# 3. Compute embeddings\n",
        "embeddings = model.encode(sentences)\n",
        "\n",
        "# 4. Apply PCA to reduce to 2D for visualization\n",
        "pca = PCA(n_components=2)\n",
        "reduced_embeddings = pca.fit_transform(embeddings)\n",
        "\n",
        "# 5. Plot the PCA results\n",
        "plt.figure(figsize=(12, 8))\n",
        "colors = ['red', 'green', 'blue',  'orange', 'cyan']  # Unique color for each category\n",
        "categories = ['Greetings', 'Sports', 'Questions', 'Technology', 'Food']  # Just an example\n",
        "\n",
        "category_size = 3  # Number of sentences per category\n",
        "num_categories = len(sentences) // category_size\n",
        "\n",
        "for i in range(num_categories):\n",
        "    start_idx = i * category_size\n",
        "    end_idx = start_idx + category_size\n",
        "    cluster = reduced_embeddings[start_idx:end_idx]\n",
        "    label_group = sentences[start_idx:end_idx]\n",
        "    color = colors[i % len(colors)]\n",
        "    plt.scatter(cluster[:, 0], cluster[:, 1], color=color, label=categories[i])\n",
        "    for j, txt in enumerate(label_group):\n",
        "        plt.annotate(txt, (cluster[j, 0], cluster[j, 1]), fontsize=9, alpha=0.7)\n",
        "\n",
        "plt.title(\"PCA of Sentence Embeddings: Urdu Students and Pakistani Culture\")\n",
        "plt.xlabel(\"PCA Component 1\")\n",
        "plt.ylabel(\"PCA Component 2\")\n",
        "plt.legend(title=\"Categories\")\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## Slide 8: What is a Vector Store & RAG?\n",
        "\n",
        "**Indexing & Vector Stores**\n",
        "- After embeddings are created, we can store them in a **vector store**.\n",
        "- A **vector store** (e.g., Chroma) allows efficient similarity searches.\n",
        "\n",
        "**RAG (Retrieval-Augmented Generation)**\n",
        "- Combine a language model with external knowledge.\n",
        "- Retrieve relevant documents, feed them to the LLM as context.\n",
        "- The LLM then generates a final answer with that context.\n",
        "\n",
        "---\n",
        "\n",
        "## Slide 9: Build a Simple RAG\n",
        "\n",
        "```python\n",
        "import requests\n",
        "import json\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_community.embeddings import GPT4AllEmbeddings\n",
        "\n",
        "urls = [\n",
        "     \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n",
        "     \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\",\n",
        "]\n",
        "\n",
        "docs = [WebBaseLoader(url).load() for url in urls]\n",
        "docs_list = [item for sublist in docs for item in sublist]\n",
        "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
        "    chunk_size=250,\n",
        "    chunk_overlap=50\n",
        ")\n",
        "doc_splits = text_splitter.split_documents(docs_list)\n",
        "\n",
        "vectorstore = Chroma.from_documents(\n",
        "    documents=doc_splits,\n",
        "    collection_name=\"rag-chroma1\",\n",
        "    embedding=GPT4AllEmbeddings(),\n",
        ")\n",
        "\n",
        "retriever = vectorstore.as_retriever(k=5)\n",
        "query = \"What are the critical features of a generative AI-powered autonomous agent system?\"\n",
        "documents = retriever.invoke(query)\n",
        "context = \"\\n\\n\".join([f\"Document {i+1}:\\n{doc.page_content}\" for i, doc in enumerate(documents)])\n",
        "\n",
        "prompt = f\"\"\"\n",
        "You are an expert in Generative AI and autonomous agent systems.\n",
        "Below is the context retrieved from relevant documents.\n",
        "Use this context to provide a detailed and accurate answer to the user's query.\n",
        "If you don't know the answer, say you don't know.\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Query:\n",
        "{query}\n",
        "\n",
        "Answer:\n",
        "\"\"\"\n",
        "\n",
        "url = 'http://localhost:11434/api/generate'\n",
        "payload = {\n",
        "    \"model\": \"llama3.2:3b\",\n",
        "    \"prompt\": prompt,\n",
        "    \"num_predict\": 2000,\n",
        "    \"temperature\": 0.0,\n",
        "    \"stream\": True\n",
        "}\n",
        "\n",
        "response = requests.post(url, json=payload, stream=True)\n",
        "\n",
        "if response.status_code == 200:\n",
        "    print(\"Model Response:\")\n",
        "    assembled_response = \"\"\n",
        "    for line in response.iter_lines(decode_unicode=True):\n",
        "        if line.strip():\n",
        "            try:\n",
        "                data = json.loads(line)\n",
        "                if \"response\" in data:\n",
        "                    assembled_response += data[\"response\"]\n",
        "                    print(data[\"response\"], end='', flush=True)\n",
        "                if data.get(\"done\", False):\n",
        "                    break\n",
        "            except json.JSONDecodeError as e:\n",
        "                print(f\"Error decoding JSON: {e}\")\n",
        "else:\n",
        "    print(f\"Error: {response.status_code} - {response.text}\")\n",
        "```\n",
        "\n",
        "---\n",
        "## Slide 9: Build a Simple RAG with Langchain\n",
        "\n",
        "```python\n",
        "import bs4\n",
        "from langchain import hub\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_community.embeddings import GPT4AllEmbeddings\n",
        "import os\n",
        "os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
        "os.environ['LANGCHAIN_ENDPOINT'] = 'https://api.smith.langchain.com'\n",
        "os.environ['LANGCHAIN_API_KEY'] = \"<insert your key here>\"\n",
        "os.environ[\"GROQ_API_KEY\"] = \"<insert your key here>\"\n",
        "os.environ[\"LANGSMITH_API_KEY\"] = \"<insert your key here>\"\n",
        "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
        "os.environ[\"HF_TOKEN\"] = \"<insert your key here>\"\n",
        "\n",
        "from langchain_groq import ChatGroq\n",
        "llm = ChatGroq(\n",
        "    model=\"llama-3.3-70b-versatile\",#\"mixtral-8x7b-32768\",\n",
        "    temperature=0,\n",
        "    max_tokens=None,\n",
        "    timeout=None,\n",
        "    max_retries=2,\n",
        ")\n",
        "chat = llm\n",
        "\n",
        "# INDEXING\n",
        "loader = WebBaseLoader(\n",
        "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
        "    bs_kwargs=dict(\n",
        "        parse_only=bs4.SoupStrainer(\n",
        "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
        "        )\n",
        "    ),\n",
        ")\n",
        "docs = loader.load()\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "splits = text_splitter.split_documents(docs)\n",
        "vectorstore = Chroma.from_documents(documents=splits, embedding=GPT4AllEmbeddings())\n",
        "retriever = vectorstore.as_retriever()\n",
        "\n",
        "# Prompt from LangChain Hub\n",
        "prompt = hub.pull(\"rlm/rag-prompt\")\n",
        "\n",
        "def format_docs(docs):\n",
        "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "rag_chain = (\n",
        "    {\n",
        "        \"context\": retriever | format_docs,\n",
        "        \"question\": RunnablePassthrough()\n",
        "    }\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "print(rag_chain.invoke(\"What is Task Decomposition?\"))\n",
        "```\n",
        "---\n",
        "\n",
        "## Slide 10: Using LangChain Chains\n",
        "\n",
        "```python\n",
        "from langchain.chains import SequentialChain\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "from langchain_groq import ChatGroq\n",
        "\n",
        "llm = ChatGroq(\n",
        "    model=\"llama-3.3-70b-versatile\",\n",
        "    temperature=0.9,\n",
        "    max_tokens=400,\n",
        "    verbose=True,\n",
        ")\n",
        "\n",
        "first_prompt = ChatPromptTemplate.from_template(\n",
        "    \"Translate the following review to English:\\n\\n{Review}\"\n",
        ")\n",
        "chain_one = LLMChain(llm=llm, prompt=first_prompt, output_key=\"English_Review\")\n",
        "\n",
        "second_prompt = ChatPromptTemplate.from_template(\n",
        "    \"Can you summarize the following review in 1 sentence:\\n\\n{English_Review}\"\n",
        ")\n",
        "chain_two = LLMChain(llm=llm, prompt=second_prompt, output_key=\"summary\")\n",
        "\n",
        "third_prompt = ChatPromptTemplate.from_template(\n",
        "    \"What language is the following review:\\n\\n{Review}\"\n",
        ")\n",
        "chain_three = LLMChain(llm=llm, prompt=third_prompt, output_key=\"language\")\n",
        "\n",
        "fourth_prompt = ChatPromptTemplate.from_template(\n",
        "    \"Write a follow up response to the following summary in the specified language:\\n\\nSummary: {summary}\\nLanguage: {language}\"\n",
        ")\n",
        "chain_four = LLMChain(llm=llm, prompt=fourth_prompt, output_key=\"followup_message\")\n",
        "\n",
        "fifth_prompt = ChatPromptTemplate.from_template(\n",
        "    \"Translate the followup message to English language:\\n\\n{followup_message}\"\n",
        ")\n",
        "chain_five = LLMChain(llm=llm, prompt=fifth_prompt, output_key=\"english_translation\")\n",
        "\n",
        "overall_chain = SequentialChain(\n",
        "    chains=[chain_one, chain_two, chain_three, chain_four, chain_five],\n",
        "    input_variables=[\"Review\"],\n",
        "    output_variables=[\"English_Review\", \"summary\", \"language\", \"followup_message\", \"english_translation\"],\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "review = \"میں نے کنگ سائز سیٹ آرڈر کیا تھا۔ میری واحد تنقید یہ ہوگی کہ کاش بیچنے والا کنگ سائز سیٹ کے ساتھ چار تکیے کے غلاف فراہم کرتا...\"\n",
        "\n",
        "results = overall_chain(review)\n",
        "results\n",
        "```\n",
        "\n",
        "---\n",
        "## Slide 11: LangChain Memory\n",
        "\n",
        "```python\n",
        "from langchain.chains import ConversationChain\n",
        "from langchain.memory import ConversationSummaryBufferMemory\n",
        "from langchain_groq import ChatGroq\n",
        "\n",
        "llm = ChatGroq(model=\"llama-3.3-70b-versatile\", temperature=0.9)\n",
        "memory = ConversationSummaryBufferMemory(llm=llm, max_token_limit=100)\n",
        "\n",
        "# Store sample conversation\n",
        "memory.save_context({\"input\": \"Hello\"}, {\"output\": \"What's up\"})\n",
        "memory.save_context({\"input\": \"Not much, just hanging\"}, {\"output\": \"Cool\"})\n",
        "conversation = ConversationChain(llm=llm, memory=memory, verbose=True)\n",
        "response = conversation.predict(input=\"What is on the schedule today?\")\n",
        "print(response)\n",
        "\n",
        "# Example 2\n",
        "schedule = \"At 6:30 AM, you have a GEN AI Practical Presentation...\"\n",
        "memory = ConversationSummaryBufferMemory(llm=llm, max_token_limit=100)\n",
        "memory.save_context({\"input\": \"Hello\"}, {\"output\": \"What's up\"})\n",
        "memory.save_context({\"input\": \"Not much, just hanging\"}, {\"output\": \"Cool\"})\n",
        "memory.save_context({\"input\": \"What is on the schedule today?\"}, {\"output\": f\"{schedule}\"})\n",
        "memory.load_memory_variables({})\n",
        "conversation = ConversationChain(llm=llm, memory=memory, verbose=True)\n",
        "print(conversation.predict(input=\"suggest some good questions to ask about genAI in practice?\"))\n",
        "```\n",
        "\n",
        "---\n",
        "## Slide 12: Build “Habib” Conversational Agent (Example)\n",
        "\n",
        "```python\n",
        "import re\n",
        "import json\n",
        "import requests\n",
        "\n",
        "class HabibUniversity:\n",
        "    def __init__(self, model = \"llama3.2:1b\", temperature=0):\n",
        "        \"\"\"\n",
        "        Initialize the ChatOllama object with the specified model and temperature.\n",
        "        \"\"\"\n",
        "        self.model = model\n",
        "        self.temperature = temperature\n",
        "        self.url = 'http://localhost:11434/api/generate'\n",
        "        self.system_prompt = \"\"\"\n",
        "        Your name is Habib-Pro. You are a knowledgeable and empathetic virtual assistant for Habib University...\n",
        "        \"\"\"\n",
        "\n",
        "    def generate(self, prompt):\n",
        "        full_prompt = f\"{self.system_prompt}\\n\\nUser: {prompt}\\nAssistant:\"\n",
        "        payload = {\n",
        "            \"model\": self.model,\n",
        "            \"prompt\": full_prompt,\n",
        "            \"temperature\": self.temperature\n",
        "        }\n",
        "        response = requests.post(self.url, json=payload)\n",
        "        return self._handle_response(response)\n",
        "\n",
        "    def _handle_response(self, response):\n",
        "        assembled_response = \"\"\n",
        "        for line in response.iter_lines(decode_unicode=True):\n",
        "            if line.strip():\n",
        "                try:\n",
        "                    data = json.loads(line)\n",
        "                    if \"response\" in data:\n",
        "                        assembled_response += data[\"response\"]\n",
        "                        print(data[\"response\"], end='', flush=True)\n",
        "                    if data.get(\"done\", False):\n",
        "                        break\n",
        "                except json.JSONDecodeError as e:\n",
        "                    print(f\"Error decoding JSON: {e}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    local_llm = \"llama3.2:1b\"\n",
        "    llm = HabibUniversity(model=local_llm, temperature=0)\n",
        "    prompt = \"what's your name\"\n",
        "    response = llm.generate(prompt)\n",
        "    print(response)\n",
        "```\n",
        "\n",
        "---\n",
        "## Putting It All Together\n",
        "\n",
        "- **Embeddings** represent text in dense vector form.\n",
        "- **Vector Stores** like Chroma store these vectors.\n",
        "- **RAG** combines external knowledge with LLM prompts.\n",
        "- **LangChain** orchestrates multi-step processes.\n",
        "- **Local/Open-Source LLMs** can be integrated if you have the server.\n",
        "\n",
        "**End of Notebook**\n",
        "\n",
        "---  \n",
        "\n",
        "### Thank You!\n",
        "\n",
        "- Questions?\n",
        "\n",
        "```\n",
        "```"
      ],
      "metadata": {
        "id": "rcMd672WFqY_"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "stpTiLzGFuo0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}